{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sensible_raw.loaders import loader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.svm import OneClassSVM\n",
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from build_dataset.extractors.sms_extractor import Sms_extractor\n",
    "from build_dataset.extractors.stop_locations_extractor import Stop_locations_extractor\n",
    "from build_dataset.extractors.screen_extractor import Screen_extractor\n",
    "##from facebook_friends_extractor import Facebook_friends_extractor\n",
    "from build_dataset.extractors.bluetooth_extractor import Bluetooth_extractor\n",
    "##from calllog_extractor import Calllog_extractor\n",
    "##from location_extractor import Location_extractor\n",
    "from build_dataset.extractors.big_five_extractor import Big_five_extractor\n",
    "\n",
    "from build_dataset.analysis.outlier_detection import Outlier_detector_svm, Outlier_detector_kd\n",
    "from build_dataset.analysis.location_reference import Load_location_reference\n",
    "#from analysis.social_state_reference import Load_social_state_reference\n",
    "from build_dataset.analysis.consensus_archetypes import Consensus_archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pareto_clustering.cluster.build_S import Build_S\n",
    "#from pareto_clustering.cluster import cluster_Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tc0: School periods (when there are lectures)\n",
    "# tc1: Exam periods\n",
    "# tc2: Holiday periods\n",
    "\n",
    "tc0 = {'hours': range(24), 'days': range(7), 'spans': [(\"06/01/14\",\"24/01/14\"), (\"03/02/14\",\"16/05/14\"), (\"01/09/14\",\"05/12/14\"), (\"02/06/14\",\"20/06/14\")]}\n",
    "tc1 = {'hours': range(24), 'days': range(7), 'spans': [(\"17/05/14\",\"01/06/14\"), (\"06/12/14\", \"21/12/14\")]}\n",
    "tc2 = {'hours': range(24), 'days': range(7), 'spans': [(\"01/01/14\",\"05/01/14\"), (\"25/01/14\",\"02/02/14\"), (\"14/04/14\",\"20/04/14\"), (\"21/06/14\",\"30/08/14\"), (\"22/12/14\", \"31/12/14\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[location_reference] Loading datasource from local.\n"
     ]
    }
   ],
   "source": [
    "location_reference = Load_location_reference(tc1, auxlabel=\"tc1_\", load_reference=True)\n",
    "#social_state_reference = Load_social_state_reference(tc1, auxlabel=\"tc1_\", load_reference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sms] Loading datasource from local.\n",
      "[sms] Number of datapoints in range: 1786225\n",
      "[stop_locations] Loading datasource from local.\n",
      "[stop_locations] Number of datapoints in range: 450662\n",
      "[screen] Loading datasource from local.\n",
      "[screen] Number of datapoints in range: 20211170\n",
      "[bluetooth] Loading datasource from local.\n",
      "[bluetooth] Number of datapoints in range: 3312659\n",
      "[big_five_extractor] Loaded data from local copy!\n"
     ]
    }
   ],
   "source": [
    "load_datasources_from_local = True\n",
    "\n",
    "sms = Sms_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "stop_locations = Stop_locations_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "screen = Screen_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "##facebook_friends = Facebook_friends_extractor()\n",
    "bluetooth = Bluetooth_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "##calllog = Calllog_extractor()\n",
    "##location = Location_extractor()\n",
    "big_five = Big_five_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tc0 = {'hours': range(24), 'days': range(7), 'spans': [(\"06/01/14\",\"24/01/14\"), (\"03/02/14\",\"16/05/14\"), (\"01/09/14\",\"05/12/14\"), (\"02/06/14\",\"20/06/14\")]} #in school\n",
    "#location_reference = Load_location_reference(tc1, auxlabel=\"tc1_\", load_reference=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build full JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_json_dataset(load_cached_data_sets=True):\n",
    "    \"\"\"Build json dataset with key for every user\n",
    "    \n",
    "    Loop over user-ids and for each one, collect features from the\n",
    "    extractors. That's basically it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    load_cached_data_sets : bool\n",
    "        Option to load prebuilt json datasets and just return those\n",
    "        or to build them from scratch again. Note that building from\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect a list of valid user-ids\n",
    "    with open('build_dataset/data_cache/users.json', 'r') as infile:\n",
    "        users = [int(i) for i in json.load(infile)]\n",
    "\n",
    "    if load_cached_data_sets:\n",
    "        with open('build_dataset/data_cache/dataset_X.json') as infile:\n",
    "            dataset_X = json.load(infile)\n",
    "        with open('build_dataset/data_cache/dataset_Y.json') as infile:\n",
    "            dataset_Y = json.load(infile)\n",
    "    else:\n",
    "        dataset_X = {}\n",
    "        dataset_Y = {}\n",
    "\n",
    "        for user in users:\n",
    "\n",
    "            if user%10 == 0:\n",
    "                print user,\n",
    "\n",
    "            datapoint_x = {}\n",
    "            datapoint_y = {}\n",
    "\n",
    "            # Ordered by fail/execution speed\n",
    "            try:\n",
    "                datapoint_x.update(bluetooth.main(user))\n",
    "                datapoint_x.update(stop_locations.main(user))\n",
    "                datapoint_x.update(sms.main(user))\n",
    "                datapoint_x.update(screen.main(user))\n",
    "                #datapoint_x.update(facebook_friends.main(user))\n",
    "                #datapoint_x.update(calllog.main(user))\n",
    "                #datapoint_x.update(location.main(user))\n",
    "                datapoint_y.update(big_five.main(user))\n",
    "            except Exception as e:\n",
    "                print \"<\"+str(e)+\">\",\n",
    "                continue\n",
    "\n",
    "            dataset_X[user] = datapoint_x\n",
    "            dataset_Y[user] = datapoint_y\n",
    "\n",
    "        # Store loaded data    \n",
    "        with open('build_dataset/data_cache/dataset_X.json', 'w') as outfile:\n",
    "            json.dump(dataset_X,outfile)\n",
    "        with open('build_dataset/data_cache/dataset_Y.json', 'w') as outfile:\n",
    "            json.dump(dataset_Y,outfile)  \n",
    "    \n",
    "    return dataset_X, dataset_Y\n",
    "\n",
    "\n",
    "dataset_X, dataset_Y = build_json_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to matrix and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_x = sorted(dataset_X.items()[0][1].keys())\n",
    "features_y = ['openness', 'conscientiousness', 'extraversion', 'aggreeableness', 'neuroticism']\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for k,v in dataset_X.items():\n",
    "    X.append([v[f] for f in features_x])\n",
    "for k,v in dataset_Y.items():\n",
    "    Y.append([v[f] for f in features_y])\n",
    "    \n",
    "X_scaled = scale(np.array(X))\n",
    "Y = np.array(Y)\n",
    "M = Consensus_archetypes().project_to_archetype_space(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'tc0_bluetooth_social_entropy',\n",
       " 1: u'tc0_screen_session_duration',\n",
       " 2: u'tc0_screen_session_frequency',\n",
       " 3: u'tc0_screen_summed_usage',\n",
       " 4: u'tc0_sms_fractions_of_conversations_started',\n",
       " 5: u'tc0_sms_overall_received_responsiveness',\n",
       " 6: u'tc0_sms_overall_responsiveness',\n",
       " 7: u'tc0_sms_selectivity_in_responsiveness',\n",
       " 8: u'tc0_sms_traffic',\n",
       " 9: u'tc0_stop_locations_geospacial_entropy'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature reference \n",
    "dict(zip(range(len(features_x)),features_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 50 outliers, clean subset has 577 samples\n"
     ]
    }
   ],
   "source": [
    "#out_svm = Outlier_detector_svm(X_scaled[:,[5,7]], hard=False, threshold=-1, visualize=False, nu=0.1, gamma=0.25)\n",
    "out_kd = Outlier_detector_kd(X_scaled, visualize=False, threshold=0.08, bandwidth=2, kernel='gaussian')\n",
    "outliers = out_kd.main()\n",
    "\n",
    "X_clean = np.delete(X_scaled,outliers,axis=0)\n",
    "Y_clean = np.delete(Y,outliers,axis=0)\n",
    "M_clean = np.delete(M,outliers,axis=0)\n",
    "\n",
    "print \"Removed %d outliers, clean subset has %d samples\" % (\n",
    "    (X_scaled.shape[0]-X_clean.shape[0]), X_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/X.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a1649709b2af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/X.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/Y.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/X_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/Y_clean.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/M.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ulf/anaconda2/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[0;32m   1094\u001b[0m                 \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m                 \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/X.csv'"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"data/X.csv\", X_scaled, delimiter=\",\")\n",
    "np.savetxt(\"data/Y.csv\", Y, delimiter=\",\")\n",
    "np.savetxt(\"data/X_clean.csv\", X_clean, delimiter=\",\")\n",
    "np.savetxt(\"data/Y_clean.csv\", Y_clean, delimiter=\",\")\n",
    "np.savetxt(\"data/M.csv\", M, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, T, _ = Build_S(X_scaled,10,sample_size=1.0, remove_outliers=True).main(visualize=False)\n",
    "clusters = cluster_Infomap.community_detection_Infomap(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = []\n",
    "for clu, traits in clusters.items():\n",
    "    Xc = X_clean[:,traits]\n",
    "    X_all.append(Xc)\n",
    "    \n",
    "for i, Xc in enumerate(X_all):\n",
    "    np.savetxt(\"data/X%s.csv\" % i, Xc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbuild_dataset\u001b[0m/  __init__.py  main.ipynb  \u001b[01;34mpareto_clustering\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 2",
   "language": "python",
   "name": "anaconda2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
