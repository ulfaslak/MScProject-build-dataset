{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import json\n",
    "from sensible_raw.loaders import loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1. Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from build_dataset.extractors.sms_extractor import Sms_extractor\n",
    "from build_dataset.extractors.bandicoot_extractor import Bandicoot_extractor\n",
    "from build_dataset.extractors.stop_locations_extractor import Stop_locations_extractor\n",
    "from build_dataset.extractors.screen_extractor import Screen_extractor\n",
    "##from facebook_friends_extractor import Facebook_friends_extractor\n",
    "from build_dataset.extractors.bluetooth_extractor import Bluetooth_extractor\n",
    "##from calllog_extractor import Calllog_extractor\n",
    "##from location_extractor import Location_extractor\n",
    "from build_dataset.extractors.big_five_extractor import Big_five_extractor\n",
    "\n",
    "from build_dataset.analysis.outlier_detection import Outlier_detector_svm, Outlier_detector_kd\n",
    "from build_dataset.analysis.location_reference import Load_location_reference\n",
    "#from analysis.social_state_reference import Load_social_state_reference\n",
    "from build_dataset.analysis.consensus_archetypes import Consensus_archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tc0: School periods (when there are lectures)\n",
    "# tc1: Exam periods\n",
    "# tc2: Holiday periods\n",
    "\n",
    "tc0 = {'hours': range(24), 'days': range(7), 'spans': [(\"06/01/14\",\"24/01/14\"), (\"03/02/14\",\"16/05/14\"), (\"01/09/14\",\"05/12/14\"), (\"02/06/14\",\"20/06/14\")]}\n",
    "tc1 = {'hours': range(24), 'days': range(7), 'spans': [(\"17/05/14\",\"01/06/14\"), (\"06/12/14\", \"21/12/14\")]}\n",
    "tc2 = {'hours': range(24), 'days': range(7), 'spans': [(\"01/01/14\",\"05/01/14\"), (\"25/01/14\",\"02/02/14\"), (\"14/04/14\",\"20/04/14\"), (\"21/06/14\",\"30/08/14\"), (\"22/12/14\", \"31/12/14\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[location_reference] Loading datasource from local.\n"
     ]
    }
   ],
   "source": [
    "location_reference = Load_location_reference(tc0, auxlabel=\"tc0_\", load_reference=True)\n",
    "#social_state_reference = Load_social_state_reference(tc0, auxlabel=\"tc0_\", load_reference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sms] Loading datasource from local.\n",
      "[sms] Number of datapoints in range: 1786225\n",
      "[stop_locations] Loading datasource from local.\n",
      "[stop_locations] Number of datapoints in range: 450662\n",
      "[screen] Loading datasource from local.\n",
      "[screen] Number of datapoints in range: 20211170\n",
      "[bluetooth] Loading datasource from local.\n",
      "[bluetooth] Number of datapoints in range: 3312659\n",
      "[big_five_extractor] Loaded data from local copy!\n"
     ]
    }
   ],
   "source": [
    "load_datasources_from_local = True\n",
    "\n",
    "sms = Sms_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "bandicoot = Bandicoot_extractor(tc0, supress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "stop_locations = Stop_locations_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "screen = Screen_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "##facebook_friends = Facebook_friends_extractor()\n",
    "bluetooth = Bluetooth_extractor(tc0, suppress=[], auxlabel=\"tc0_\", load_old_datasources=load_datasources_from_local)\n",
    "##calllog = Calllog_extractor()\n",
    "##location = Location_extractor()\n",
    "big_five = Big_five_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tc0 = {'hours': range(24), 'days': range(7), 'spans': [(\"06/01/14\",\"24/01/14\"), (\"03/02/14\",\"16/05/14\"), (\"01/09/14\",\"05/12/14\"), (\"02/06/14\",\"20/06/14\")]} #in school\n",
    "#location_reference = Load_location_reference(tc1, auxlabel=\"tc1_\", load_reference=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build full JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_json_dataset(load_cached_data_sets=True):\n",
    "    \"\"\"Build json dataset with key for every user\n",
    "    \n",
    "    Loop over user-ids and for each one, collect features from the\n",
    "    extractors. That's basically it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    load_cached_data_sets : bool\n",
    "        Option to load prebuilt json datasets and just return those\n",
    "        or to build them from scratch again. Note that building from\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect a list of valid user-ids\n",
    "    with open('build_dataset/data_cache/users.json', 'r') as infile:\n",
    "        users = [int(i) for i in json.load(infile)]\n",
    "\n",
    "    if load_cached_data_sets:\n",
    "        with open('build_dataset/data_cache/dataset_X.json') as infile:\n",
    "            dataset_X = json.load(infile)\n",
    "        with open('build_dataset/data_cache/dataset_Y.json') as infile:\n",
    "            dataset_Y = json.load(infile)\n",
    "    else:\n",
    "        dataset_X = {}\n",
    "        dataset_Y = {}\n",
    "\n",
    "        for user in users:\n",
    "\n",
    "            if user%10 == 0:\n",
    "                print user,\n",
    "\n",
    "            datapoint_x = {}\n",
    "            datapoint_y = {}\n",
    "\n",
    "            # Ordered by fail/execution speed\n",
    "            try:\n",
    "                datapoint_x.update(bluetooth.main(user))\n",
    "                datapoint_x.update(stop_locations.main(user))\n",
    "                datapoint_x.update(sms.main(user))\n",
    "                datapoint_x.update(screen.main(user))\n",
    "                #datapoint_x.update(facebook_friends.main(user))\n",
    "                #datapoint_x.update(calllog.main(user))\n",
    "                #datapoint_x.update(location.main(user))\n",
    "                datapoint_y.update(big_five.main(user))\n",
    "            except Exception as e:\n",
    "                print \"<\"+str(e)+\">\",\n",
    "                continue\n",
    "\n",
    "            dataset_X[user] = datapoint_x\n",
    "            dataset_Y[user] = datapoint_y\n",
    "\n",
    "        # Store loaded data    \n",
    "        with open('build_dataset/data_cache/dataset_X.json', 'w') as outfile:\n",
    "            json.dump(dataset_X,outfile)\n",
    "        with open('build_dataset/data_cache/dataset_Y.json', 'w') as outfile:\n",
    "            json.dump(dataset_Y,outfile)\n",
    "    \n",
    "    return dataset_X, dataset_Y\n",
    "\n",
    "\n",
    "dataset_X, dataset_Y = build_json_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to matrix and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_x = sorted(dataset_X.items()[0][1].keys())\n",
    "features_y = ['openness', 'conscientiousness', 'extraversion', 'aggreeableness', 'neuroticism']\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for k,v in dataset_X.items():\n",
    "    X.append([v[f] for f in features_x])\n",
    "for k,v in dataset_Y.items():\n",
    "    Y.append([v[f] for f in features_y])\n",
    "    \n",
    "X_scaled = scale(np.array(X))\n",
    "Y = np.array(Y)\n",
    "M = Consensus_archetypes().project_to_archetype_space(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'tc0_bluetooth_social_entropy',\n",
       " 1: u'tc0_screen_session_duration',\n",
       " 2: u'tc0_screen_session_frequency',\n",
       " 3: u'tc0_screen_summed_usage',\n",
       " 4: u'tc0_sms_fractions_of_conversations_started',\n",
       " 5: u'tc0_sms_overall_received_responsiveness',\n",
       " 6: u'tc0_sms_overall_responsiveness',\n",
       " 7: u'tc0_sms_selectivity_in_responsiveness',\n",
       " 8: u'tc0_sms_traffic',\n",
       " 9: u'tc0_stop_locations_geospacial_entropy'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature reference \n",
    "dict(zip(range(len(features_x)),features_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 50 outliers, clean subset has 577 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulf/anaconda2/lib/python2.7/site-packages/numpy/lib/function_base.py:3875: FutureWarning: in the future negative indices will not be ignored by `numpy.delete`.\n",
      "  \"`numpy.delete`.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#out_svm = Outlier_detector_svm(X_scaled[:,[5,7]], hard=False, threshold=-1, visualize=False, nu=0.1, gamma=0.25)\n",
    "out_kd = Outlier_detector_kd(X_scaled, visualize=False, threshold=0.08, bandwidth=2, kernel='gaussian')\n",
    "outliers = out_kd.main()\n",
    "\n",
    "X_clean = np.delete(X_scaled,outliers,axis=0)\n",
    "Y_clean = np.delete(Y,outliers,axis=0)\n",
    "M_clean = np.delete(M,outliers,axis=0)\n",
    "\n",
    "print \"Removed %d outliers, clean subset has %d samples\" % (\n",
    "    (X_scaled.shape[0]-X_clean.shape[0]), X_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"data/X.csv\", X, delimiter=\",\")\n",
    "np.savetxt(\"data/X_scaled.csv\", X_scaled, delimiter=\",\")\n",
    "np.savetxt(\"data/Y.csv\", Y, delimiter=\",\")\n",
    "np.savetxt(\"data/X_clean.csv\", X_clean, delimiter=\",\")\n",
    "np.savetxt(\"data/Y_clean.csv\", Y_clean, delimiter=\",\")\n",
    "np.savetxt(\"data/M.csv\", M, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Pareto clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named p2t",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-739b5e24e5b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_S\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBuild_S\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster_Infomap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster_DBSCAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ulfaslak@gmail.com/backup/pipeline_code/pareto_clustering/cluster/build_S.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint_location\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmin_triangle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint_location\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshapes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint_location\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtoNumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvexHull\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpareto_clustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint_location\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutlier_detection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutlier_detector_svm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ulfaslak@gmail.com/backup/pipeline_code/pareto_clustering/dependencies/point_location/min_triangle.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshapes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTriangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mccw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvexHull\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandomConvexPolygon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ulfaslak@gmail.com/backup/pipeline_code/pareto_clustering/dependencies/point_location/geo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!/usr/bin/env python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ulfaslak@gmail.com/backup/pipeline_code/pareto_clustering/dependencies/point_location/geo/shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdrawer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ulfaslak@gmail.com/backup/pipeline_code/pareto_clustering/dependencies/point_location/geo/spatial.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mp2t\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCDT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named p2t"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from pareto_clustering.cluster.build_S import Build_S\n",
    "from pareto_clustering.cluster import cluster_Infomap\n",
    "from pareto_clustering.cluster import cluster_DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"data/X.csv\", delimiter=\",\")\n",
    "X_scaled = np.loadtxt(\"data/X_scaled.csv\", delimiter=\",\")\n",
    "Y = np.loadtxt(\"data/Y.csv\", delimiter=\",\")\n",
    "X_clean = np.loadtxt(\"data/X_clean.csv\", delimiter=\",\")\n",
    "Y_clean = np.loadtxt(\"data/Y_clean.csv\", delimiter=\",\")\n",
    "M = np.loadtxt(\"data/M.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 clusters and 0 outliers\n",
      "2 clusters and 1 outliers\n",
      "2 clusters and 2 outliers\n",
      "2 clusters and 3 outliers\n",
      "2 clusters and 4 outliers\n",
      "2 clusters and 5 outliers\n",
      "2 clusters and 6 outliers\n",
      "\t... found 1 valid solutions, using eps=1.802122, min_samples=2 (minimal params)\n"
     ]
    }
   ],
   "source": [
    "_, T, _ = Build_S(X_scaled,10,sample_size=1.0, remove_outliers=True).main(visualize=False)\n",
    "clusters = cluster_Infomap.fit(T)\n",
    "clusters = cluster_DBSCAN.fit(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all = []\n",
    "for clu, traits in clusters.items():\n",
    "    Xc = X_clean[:,np.array(traits)-1]\n",
    "    X_all.append(Xc)\n",
    "    \n",
    "for i, Xc in enumerate(X_all):\n",
    "    np.savetxt(\"data/X%s.csv\" % i, Xc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <br>\n",
    "  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tc0_[sms]_concluded_percent': 0.4672435105067985,\n",
       " 'tc0_[sms]_initiated_percent': 0.3980222496909765,\n",
       " 'tc0_[sms]_outgoing_percent': 0.46486817903126915,\n",
       " 'tc0_[sms]_responsiveness': -9.3898049498295872,\n",
       " 'tc0_[sms]_responsiveness_received': -9.0214351472865584,\n",
       " 'tc0_[sms]_responsiveness_std': -4.476345222107982,\n",
       " 'tc0_[sms]_traffic': 9.0063865150551141}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.main(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
